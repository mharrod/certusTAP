# Testing 

Below is the proposed testing plan for the PoC. Not all testing will be done on day one, but most of the testing categories should be in use before the end of the "Fly Phase".

---

##  Foundational Baselines

**Goal:** Establish reproducibility and consistent environments for all assurance workflows.

**Approach:**

- Validate environment reproducibility (`make test-env`, `dagger up`).
- Snapshot tool versions for reproducibility (`bandit --version`, `trivy --version`, etc.).
- Run dependency hygiene checks: `poetry check`, `pip-audit`.
- Verify documentation build integrity with `mkdocs build --strict`.

**Expected Output:**

- `baseline.json` and `baseline.hash` signed with Cosign.
- Verification logs confirming environment consistency.
- Dependency and documentation validation results.

---

## Static & Functional Testing

**Goal:** Ensure correctness, code safety, and maintainability at rest.

**Scope:**
- Core Python modules (`ingest`, `retrieve`, `generate`, etc.)
- Security and code quality scans (SAST + linting + typing).

**Approach:**
- Run unit tests with `pytest` (coverage > 85%).
- Lint and format code: `ruff`, `flake8`, `black`.
- Perform static typing checks: `mypy`.
- Assess code complexity: `radon cc -s src/`.
- Detect unused code: `vulture`.
- Conduct Python-specific SAST: `bandit -r src/`.
- Secrets detection: `gitleaks detect`.
- Dependency CVE checks: `trivy fs .` or `grype`.

**Expected Output:**
- Test coverage and quality reports (JSON).
- Linting and complexity metrics.
- Signed SARIF output for SAST and SCA findings.
- OCI evidence artifact: `oci://org/cap/static:<commit-hash>`.

---

## Dynamic & Runtime Security Testing

**Goal:** Identify exploitable vulnerabilities in live or running environments.

**Scope:**
- FastAPI endpoints, dashboards, webhooks.
- Authentication, rate limiting, input validation.

**Approach:**
- Deploy test environment using Dagger or Tekton.
- Perform baseline DAST with OWASP ZAP.
- Run Nuclei template scans (`cves`, `exposures`, `misconfigurations`).
- Optionally run Burp Suite CI or API fuzzers.
- Include lightweight performance regression with `pytest-benchmark` or `locust`.

**Expected Output:**
- DAST and Nuclei JSON + SARIF reports.
- Runtime performance baselines.
- OCI evidence: `oci://org/cap/dynamic:<commit-hash>`.

---

## Infrastructure & IaC Security Testing

**Goal:** Validate security and compliance of infrastructure-as-code and container configurations.

**Scope:**
- Terraform, Kubernetes manifests, Dockerfiles, GitHub Actions, etc.

**Approach:**
- Policy checks: `checkov -d infra/`, `tfsec`.
- Container image scanning: `trivy image myimage:latest`.
- Configuration scanning: `trivy config .`.
- Dockerfile linting: `hadolint Dockerfile`.
- Validate Terraform and Kubernetes manifests (`terraform validate`, `kubectl apply --dry-run`).
- Apply CIS Benchmarks or custom Assurance Manifest policies.

**Expected Output:**
- IaC and container scan reports (SARIF + JSON).
- Signed Cosign attestation: `oci://org/cap/infra:<commit-hash>`.

---

## Integration & End-to-End (E2E) Testing

**Goal:** Validate the entire assurance workflow from ingestion to reporting.

**Approach:**
- Use golden repositories with known vulnerabilities as test fixtures.
- Execute `scan run --repo <fixture>`.
- Compare deterministic outputs (`report.json`, `report.md`) across runs.
- Validate output schema (Pydantic) and API contracts (Schemathesis).
- Verify signatures (Cosign) and provenance reproducibility.

**Expected Output:**
- Signed E2E attestation (`oci://org/cap/e2e:<commit-hash>`).
- Deterministic test outputs with verified signatures.

---

## Assurance Integrity & Non-Repudiation

**Goal:** Verify the trustworthiness and provenance of all assurance artifacts.

**Approach:**
- Verify Cosign signatures for reports and manifests.
- Rebuild provenance graph using In-Toto links.
- Validate Rekor transparency log entries.
- Detect expired or invalid signatures.
- Run documentation coverage audit: `interrogate`.
- Prune unused code and stale references with `vulture`.

**Expected Output:**
- Provenance verification logs.
- Signed verification evidence.
- Documentation coverage report.

---

##  AI Assurance & Evaluation

**Goal:** Validate reasoning reliability, explainability, and fairness in AI-assisted components.

**Approach:**
- Evaluate EXPLAIN/FIX/SUMMARIZE chains with DeepEval:
  - Faithfulness ≥ 0.9
  - Relevancy ≥ 0.85
  - Consistency ≥ 0.8
  - Safety: 0 unsafe outputs
- Use GuardrailsAI or Promptfoo for schema and safety validation.
- Add Pandera or Great Expectations for data consistency checks.
- Include Feature Attribution Checks (SHAP, LIME).
- Validate and sign prompt provenance.

**Expected Output:**
- DeepEval and Guardrails reports.
- Drift and reasoning deltas.
- OCI evidence: `oci://org/cap/ai-eval:<commit-hash>`.

---

##  Continuous Regression & Drift Detection

**Goal:** Detect drift in security posture, assurance logic, and AI reasoning quality.

**Approach:**
- Run scheduled continuous assurance jobs.
- Compare DeepEval metrics, SBOM diffs, manifest digests, OpenGrep and Trivy results.
- Run mutation testing (`mutmut run`) to evaluate test robustness.
- Trigger re-evaluation if thresholds exceeded.

**Expected Output:**
- Drift reports (`drift.json`).
- Mutation testing score (≥ 80% mutant kill rate).
- OCI evidence: `oci://org/cap/drift:<commit-hash>`.

---

## Human-in-the-Loop & Workflow Validation

**Goal:** Validate manual approvals, waivers, and review workflows.

**Approach:**
- Simulate analyst and approver flow interactions.
- Validate RBAC and countersignature enforcement.
- Attempt unauthorized waiver modifications.
- Include accessibility and UX compliance checks with `axe-core` or `lighthouse-ci`.
- Verify signed waiver artifacts.

**Expected Output:**
- Workflow validation log.
- Accessibility compliance report.
- OCI evidence: `oci://org/cap/hitl:<commit-hash>`.

---

##  Chaos & Resilience Testing

**Goal:** Validate the reliability and resilience of the assurance pipeline.

**Scope:**
- Registry downtime, corrupted artifacts, signer key rotation, and interrupted pipelines.

**Approach:**
- Simulate failure and recovery scenarios.
- Verify pipeline resumption and signature integrity.
- Measure recovery time and system stability.

**Expected Output:**
- Recovery and degradation reports.
- Verified integrity logs.
- OCI artifact: `oci://org/cap/chaos:<commit-hash>`.

---

## Comprehensive Quality Audit & Attestation

**Goal:** Produce a final aggregated quality and assurance attestation covering all phases.

**Approach:**
- Aggregate reports from all previous phases (lint, test, drift, mutation, AI metrics).
- Consolidate performance, complexity, and documentation results.
- Generate a unified Quality Attestation Report (JSON + Markdown).
- Sign and store in OCI registry.

**Expected Output:**
- Unified quality summary.
- Signed attestation (`oci://org/cap/quality:<commit-hash>`).

---

## Unified Metrics and Acceptance Criteria

| Metric | Tool/Source | Target | Description |
| ------- | ------------ | ------- | ------------ |
| Test Coverage | Pytest | ≥85% | Unit test completeness |
| Linting & Type Checks | Ruff, Mypy | 100% | Static code correctness |
| Complexity Threshold | Radon | ≤10 | Maintainable code |
| Mutation Score | Mutmut | ≥80% | Test robustness |
| Documentation Coverage | Interrogate | ≥80% | Code clarity |
| Performance Regression | Pytest-benchmark | <20% diff | Runtime stability |
| AI Faithfulness | DeepEval | ≥0.9 | Reasoning reliability |
| Drift Detection Latency | Pipeline logs | <24h | Responsiveness |
| Reviewer SLA | Workflow logs | ≤48h | Human validation timeliness |

---

## Governance and Reporting

- All phases emit signed evidence to `oci://org/cap/tests/<run-id>`.
- CI/CD pipelines push metrics to Prometheus/Grafana dashboards.
- Failed thresholds create automated Jira issues (P1–P4).
- Monthly reports include:
  - DeepEval reasoning metrics
  - Drift deltas
  - Mutation and coverage summaries
  - Signature freshness status

---

## Continuous Improvement and Future Enhancements

- Expand Fuzz Testing (code, prompts, manifests).
- Integrate runtime threat detection (Falco, eBPF).
- Develop adversarial LLM red-team tests.
- Add formal verification for Policy DSLs.
- Establish inter-org attestation replays for federated trust validation.

---
